<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>pxf.service.kerberos.principal</name>
        <value>gpadmin/_HOST@EXAMPLE.COM</value>
        <description>Kerberos principal pxf service should use. _HOST is replaced automatically with hostnames FQDN</description>
    </property>
    <property>
        <name>pxf.service.kerberos.keytab</name>
        <value>${pxf.base}/keytabs/pxf.service.keytab</value>
        <description>Kerberos path to keytab file owned by pxf service with permissions 0400</description>
    </property>
    <property>
        <name>pxf.service.user.impersonation</name>
        <value>true</value>
        <description>End-user identity impersonation, set to true to enable, false to disable</description>
    </property>
    <property>
        <name>pxf.service.kerberos.constrained-delegation</name>
        <value>false</value>
        <description>
            Makes user impersonation work via Kerberos constrained delegation based on S4U2Self/Proxy Kerberos extension.
            This method does not require the PXF principal to be a Hadoop proxy user, but requires the S4U2 feature
            to be enabled in an Active Directory / IPA Server. Additional configuration is needed in the
            Active Directory / IPA Server to enable the PXF principal to impersonate end users.
            Set to true to enable, false to disable.
        </description>
    </property>

    <!--
    <property>
        <name>pxf.service.user.name</name>
        <value>${user.name}</value>
        <description>

            Uncomment and set the proper value only if:

            - user impersonation is enabled and you want to use the specified
              user as a proxy on the unsecured Hadoop clusters. This is useful
              when a proxy user has already been configured on the Hadoop side,
              and you don't want to add gpadmin (the default) as a proxy user.

            - user impersonation is disabled and you want queries from all
              Greenplum users to appear on the Hadoop side as coming from the
              specified user.

        </description>
    </property>
    -->

    <!--
    <property>
        <name>pxf.fs.basePath</name>
        <value></value>
        <description>
            Sets the base path when constructing a file URI for read and write
            operations. This property MUST be configured for any server that
            accesses a file using a file:* profile.
        </description>
    </property>
    !-->

    <property>
        <name>pxf.ppd.hive</name>
        <value>true</value>
        <description>Specifies whether Predicate Pushdown feature is enabled for Hive profiles.</description>
    </property>

    <property>
        <name>pxf.sasl.connection.retries</name>
        <value>5</value>
        <description>
            Specifies the number of retries to perform when a SASL connection is refused by a Namenode
            due to 'GSS initiate failed' error.
        </description>
    </property>

    <property>
        <name>pxf.orc.write.timezone.utc</name>
        <value>true</value>
        <description>
            Specifies whether the PXF ORC writer should use UTC timezone when writing timestamp values.
            If set to false, the PXF ORC writer will use the local timezone of the PXF JVM instead.
        </description>
    </property>

    <property>
        <name>pxf.parquet.write.decimal.overflow</name>
        <value>round</value>
        <description>
            Specifies whether the PXF Parquet profiles should error out, or log out rounded values, or ignore numeric values (convert to null) that exceed maximum column precision.
            If set to error, an error will be reported when a numeric value exceeds maximum column precision and the transaction will fail,
            potentially leaving incomplete dataset in the external system.
            If set to round, a warning will be reported when a precision overflowed value can be rounded and fit in the provided precision;
            or an error will be reported if it cannot been rounded, then transaction will fail and potentially leave incomplete dataset in the external system.
            If set to ignore, no warnings about rounded values, and NULL values will be stored for numeric values that exceeds maximum column precision,
            the transaction will succeed, but the resulting dataset in the external system will have NULL values instead of the original ones where modifications took place.
        </description>
    </property>

</configuration>
